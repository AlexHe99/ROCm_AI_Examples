{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d745b4a-2c8a-471b-95dc-0386b5522d6b",
   "metadata": {},
   "source": [
    "# Deploy Llama Stack over AMD Instinct™ GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081f7ec-fc86-4823-94d1-89b003116543",
   "metadata": {},
   "source": [
    "A step-by-step guide on how to deploy the Llama Stack on AMD Instinct™ MI300X GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c500e-0a51-47f1-800a-b45ee43a2738",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As a leader in AI open-source innovation, Meta’s Llama series has democratized access to large language models, empowering developers worldwide. The Llama Stack—Meta’s all-in-one deployment framework—extends this vision by enabling seamless transitions from research to production through built-in tools for optimization, API integration, and scalability. This unified platform is ideal for teams requiring robust support to deploy Meta’s models at scale across diverse applications.\n",
    "\n",
    "Complementing this ecosystem, AMD reinforces its position as a leader in AI acceleration hardware by expanding the AI software frontier through the ROCm™ open-source software stack. By fostering collaboration and optimizing performance, ROCm™ equips developers with a robust foundation to build high-throughput AI solutions tailored for production environments.\n",
    "\n",
    "This tutroial guides developers in deploying the Llama Stack on AMD ROCm™-powered GPUs, creating a production-ready infrastructure for large language model (LLM) inference. We’ll also demonstrate programmatic interactions via the Llama Stack CLI and Python SDK, ensuring seamless server integration. To streamline this journey, we’ll first preview the core components involved—such as ROCm’s optimization tools, Llama Stack’s deployment workflows, and scalable GPU configurations—before diving into the hands-on session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c68b8b-3a16-424b-8341-2c587d1e0727",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Llama Stack and Remote vLLM Distribution\n",
    "\n",
    "Llama Stack defines and standardizes the core building blocks needed to bring generative AI applications to market. It provides a unified set of APIs with implementations from leading service providers, enabling seamless transitions between development and production environments. [1]\n",
    "\n",
    "  ![Llama Stack](https://llama-stack.readthedocs.io/en/latest/_images/llama-stack.png)\n",
    "\n",
    "The Llama Stack’s Inference API is interoperable with a wide range of LLM inference providers, including vLLM, TGI, Ollama, and OpenAI APIs, ensuring seamless integration and flexibility for deployment. And it also provide 4 types client SDK, Python, Swift, Node and Kotlin.\n",
    "\n",
    "For this tutorial, we’ve selected vLLM as the inference provider and the Llama Stack’s Python Client SDK to showcase scalable deployment workflows and illustrate hands-on, low-latency LLM integration into production-ready services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03e005b-43c2-4beb-b504-4154db92976b",
   "metadata": {},
   "source": [
    "### ROCm™ and vLLM docker images\n",
    "\n",
    "ROCm™ is an open-source software platform optimized to extract HPC and AI workload performance from AMD Instinct accelerators and AMD Radeon GPUs while maintaining compatibility with industry software frameworks. For more information, see [What is ROCm™?](https://ROCm™.docs.amd.com/en/latest/what-is-ROCm™.html) [2]\n",
    "\n",
    "AMD collaborates with vLLM to deliver a streamlined, high-performance LLM inference engine and production-ready deployment solutions for enterprise-grade AI workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cedc16-fcf8-470c-8bbc-33f28eae5ac6",
   "metadata": {},
   "source": [
    "**Available vLLM Containers** [3]\n",
    "\n",
    "AMD provides two main vLLM container options:\n",
    "\n",
    "- [ROCm™/vllm](https://hub.docker.com/r/ROCm™/vllm): Production-ready container\n",
    "\n",
    "  - Pinned to a specific version, for example: ROCm™/vllm-dev:ROCm™6.3.1_mi300_ubuntu22.04_py3.12_vllm_0.6.6\n",
    "\n",
    "  - Designed for stability\n",
    "\n",
    "  - Optimized for deployment\n",
    "\n",
    "- [ROCm™/vllm-dev](https://hub.docker.com/r/ROCm™/vllm-dev): Development container with the latest vLLM features\n",
    "\n",
    "  - nightly, main and other specialized builds available:\n",
    "\n",
    "    - nightly tags are built daily from the latest code, but may contain bugs\n",
    "\n",
    "    - main tags are more stable builds, updated after testing\n",
    "\n",
    "  - Includes development tools\n",
    "\n",
    "  - Best for testing new features or custom modifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f081e22-c402-4ac6-8189-06ab4f6be5f5",
   "metadata": {},
   "source": [
    "## Deployment Llama Stack with ROCm™\n",
    "\n",
    "We use [Remote vLLM Distribution](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/remote-vllm.html#remote-vllm-distribution \"Link to this heading\") running with ROCm™/vllm-dev docker image on Instinct™ MI300X GPU. In addition to supporting many LLM inference providers (e.g., Fireworks, Together, AWS Bedrock, Groq, Cerebras, SambaNova, vLLM, etc.), Llama Stack also allows users to choose safety providers as an option (e.g., Meta’s Llama Guard, AWS Bedrock Guardrails, vLLM, etc.). In this tutorial, we use two Instinct™ MI300X GPUs: one for deploying LLM inference APIs, and another for Safety/Shield APIs deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8d389-2078-4a4b-a171-ae707c156e8e",
   "metadata": {},
   "source": [
    "### Prerequist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34090516-4fad-48f9-97fd-5f19be807c7f",
   "metadata": {},
   "source": [
    "Environment of this tutorial:\n",
    "\n",
    "- GPU: AMD Instinct™ MI300x\n",
    "- OS: Ubuntu22.04\n",
    "- Docker image: rocm/vllm-dev:main (vllm: 0.7.4.dev388+g51641aaa7.rocm631 )\n",
    "- llama-stack: v0.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bae834-bbae-473d-bb84-01d0c4a003bf",
   "metadata": {},
   "source": [
    "#### Start ROCm™/vllm vLLM server container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c775d-ccc2-4b63-b5ba-9fc116262fb6",
   "metadata": {},
   "source": [
    "#set your token of Huggingface\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7e97f-2d39-4798-9989-de7c5b347e42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# or use input the token like that,\n",
    "import os\n",
    "\n",
    "# input your HF_TOKEN\n",
    "hf_token = input(\"input your Hugging Face Token: \")\n",
    "os.environ['HF_TOKEN'] = hf_token\n",
    "\n",
    "print(\"HF_TOKEN set！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8a0658-9b97-4287-8f96-0bafaf587d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1b590b7b6342bcfca3fa1e98480882183a384e98bf40160cc563d7d3055be0b7\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export INFERENCE_PORT=8080\n",
    "export INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "export VLLM_DIMG=\"rocm/vllm-dev:main\"\n",
    "docker run -d --rm\\\n",
    "    --ipc=host \\\n",
    "    --privileged \\\n",
    "    --shm-size 16g \\\n",
    "    --device=/dev/kfd \\\n",
    "    --device=/dev/dri \\\n",
    "    --group-add video \\\n",
    "    --cap-add=SYS_PTRACE \\\n",
    "    --cap-add=CAP_SYS_ADMIN \\\n",
    "    --security-opt seccomp=unconfined \\\n",
    "    --security-opt apparmor=unconfined \\\n",
    "    --env \"HUGGING_FACE_HUB_TOKEN=$HF_TOKEN\" \\\n",
    "    --env \"HIP_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\" \\\n",
    "    -p $INFERENCE_PORT:$INFERENCE_PORT \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    --name rocm-vllm-provider \\\n",
    "    $VLLM_DIMG \\\n",
    "    python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model $INFERENCE_MODEL \\\n",
    "    --port $INFERENCE_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e36df-e4de-4229-b2c3-397c3dc8f498",
   "metadata": {},
   "source": [
    "**Note** that you’ll also need to set --enable-auto-tool-choice and --tool-call-parser to enable tool calling in vLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e4743-ee28-455b-a670-3a8b2b2a8bdd",
   "metadata": {},
   "source": [
    "If you are using Llama Stack Safety / Shield APIs, then you will need to also run another instance of a vLLM with a corresponding safety model like meta-llama/Llama-Guard-3-1B using a script like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7ff7e7-2504-4b50-b378-fe7aa5a1ef25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8f924b37c2291ed9cfed9e706dd657d2bf660d106e6195c9efae2e9cc3d6740d\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export SAFETY_PORT=8081\n",
    "export SAFETY_MODEL=meta-llama/Llama-Guard-3-1B\n",
    "export CUDA_VISIBLE_DEVICES=1\n",
    "export VLLM_DIMG=\"rocm/vllm-dev:main\"\n",
    "\n",
    "docker run -d --rm\\\n",
    "    --ipc=host \\\n",
    "    --privileged \\\n",
    "    --shm-size 16g \\\n",
    "    --device=/dev/kfd \\\n",
    "    --device=/dev/dri \\\n",
    "    --group-add video \\\n",
    "    --cap-add=SYS_PTRACE \\\n",
    "    --cap-add=CAP_SYS_ADMIN \\\n",
    "    --security-opt seccomp=unconfined \\\n",
    "    --security-opt apparmor=unconfined \\\n",
    "    --env \"HUGGING_FACE_HUB_TOKEN=$HF_TOKEN\" \\\n",
    "    --env \"HIP_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\" \\\n",
    "    -p $SAFETY_PORT:$SAFETY_PORT \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    --name rocm-vllm-guard \\\n",
    "    $VLLM_DIMG \\\n",
    "    python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model $SAFETY_MODEL \\\n",
    "    --port $SAFETY_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ad487-8615-48ff-bdd7-81bee616b33c",
   "metadata": {},
   "source": [
    "It needs enough time for vllm serve to launch the LLM service. The bigger LLLM need more time to be loaded. So you may need to adjust the time regard to your environment. Another way is to start the two vllm serve containers before running the subsequent steps of this jupyter notebook and use curl test to make sure the vllm serve is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c36fc5-b3a2-46be-bc75-4b8a6ad3e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleep 360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843a98b9-fdf4-4411-88b4-d84da7a1a5ec",
   "metadata": {},
   "source": [
    "Let's test weither the two vLLM serve containers are ready. If not, you should more time by using `sleep` command in bash until you got the right response of the curl test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a613f3-b503-478b-8223-aab54ea26bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"meta-llama/Llama-3.2-3B-Instruct\",\"object\":\"model\",\"created\":1744505063,\"owned_by\":\"vllm\",\"root\":\"meta-llama/Llama-3.2-3B-Instruct\",\"parent\":null,\"max_model_len\":131072,\"permission\":[{\"id\":\"modelperm-ae9ae6b274cf42e49446d191d5102f70\",\"object\":\"model_permission\",\"created\":1744505063,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:8080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d281ce68-74a9-45bc-91a7-7381f2f6e9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "             Dload  Upload   Total   Spent    Left  Speed\n",
      "  150    229     90  0:00:01  0:00:01 --:--:--   32001  0:00:01 --:--:--   124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"cmpl-28adf0e730f24efbb5c38debb754e840\",\"object\":\"text_completion\",\"created\":1744505063,\"model\":\"meta-llama/Llama-3.2-3B-Instruct\",\"choices\":[{\"index\":0,\"text\":\" city like no other. From its\",\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null,\"prompt_logprobs\":null}],\"usage\":{\"prompt_tokens\":5,\"total_tokens\":12,\"completion_tokens\":7,\"prompt_tokens_details\":null}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl http://localhost:8080/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        \"prompt\": \"San Francisco is a\",\n",
    "        \"max_tokens\": 7,\n",
    "        \"temperature\": 0\n",
    "    }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25574c28-750f-4665-a7f0-ad1ce2f4f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"meta-llama/Llama-Guard-3-1B\",\"object\":\"model\",\"created\":1744505065,\"owned_by\":\"vllm\",\"root\":\"meta-llama/Llama-Guard-3-1B\",\"parent\":null,\"max_model_len\":131072,\"permission\":[{\"id\":\"modelperm-c3d6fb3d8f3a40f4a343fcde368cec23\",\"object\":\"model_permission\",\"created\":1744505065,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:8081/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57213854-9740-43d8-8240-4d019d0ec4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "             Dload  Upload   Total   Spent    Left  Speed\n",
      "  134  0:00:01  0:00:01 --:--:--   487    134  0:00:01  0:00:01 --:--:--   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"cmpl-0f1308e60860420dbe4206f446c704ff\",\"object\":\"text_completion\",\"created\":1744505065,\"model\":\"meta-llama/Llama-Guard-3-1B\",\"choices\":[{\"index\":0,\"text\":\" city that is full of history and\",\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null,\"prompt_logprobs\":null}],\"usage\":{\"prompt_tokens\":5,\"total_tokens\":12,\"completion_tokens\":7,\"prompt_tokens_details\":null}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    " curl http://localhost:8081/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"meta-llama/Llama-Guard-3-1B\",\n",
    "        \"prompt\": \"San Francisco is a\",\n",
    "        \"max_tokens\": 7,\n",
    "        \"temperature\": 0\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce425c80-0495-497f-8aa3-a4da3df2104c",
   "metadata": {},
   "source": [
    "### Install llama-stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b0e1d-aafd-45dd-90c6-23376c943128",
   "metadata": {},
   "source": [
    "Install llama-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f65835b0-dd94-4f25-a78f-030badb3ea41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-stack in /home/amd/.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: llama-stack-client in /home/amd/.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: blobfile in /home/amd/.local/lib/python3.10/site-packages (from llama-stack) (3.0.0)\n",
      "Requirement already satisfied: fire in /home/amd/.local/lib/python3.10/site-packages (from llama-stack) (0.7.0)\n",
      "rom llama-stack) (0.28.1)fied: httpx in /home/amd/.local/lib/python3.10/site-packages (f\n",
      "hon3.10/site-packages (from llama-stack) (0.29.3) /home/amd/.local/lib/pyt\n",
      "ome/amd/.local/lib/python3.10/site-packages (from llama-stack) (3.1.6)\n",
      ": jsonschema in /home/amd/.local/lib/python3.10/site-packages (from llama-stack) (4.23.0)\n",
      "Requirement already satisfied: prompt-toolkit in /home/amd/.local/lib/python3.10/site-packages (from llama-stack) (3.0.50)\n",
      "10/site-packages (from llama-stack) (1.0.1)v in /home/amd/.local/lib/python3.\n",
      ".local/lib/python3.10/site-packages (from llama-stack) (2.10.6)\n",
      "sts in /home/amd/.local/lib/python3.10/site-packages (from llama-stack) (2.32.3)\n",
      " satisfied: rich in /home/amd/.local/lib/python3.10/site-packages (from llama-stack) (13.9.4)\n",
      "ement already satisfied: setuptools in /usr/lib/python3/dist-packages (from llama-stack) (59.6.0)\n",
      "quirement already satisfied: termcolor in /home/amd/.local/lib/python3.10/site-packages (from llama-stack) (2.5.0)\n",
      "ges (from llama-stack) (0.9.0) tiktoken in /home/amd/.local/lib/python3.10/site-packa\n",
      ".10/site-packages (from llama-stack) (11.1.0)e/amd/.local/lib/python3\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/amd/.local/lib/python3.10/site-packages (from llama-stack-client) (4.8.0)\n",
      "0/site-packages (from llama-stack-client) (8.1.8)/.local/lib/python3.1\n",
      " /usr/lib/python3/dist-packages (from llama-stack-client) (1.7.0)\n",
      "das in /home/amd/.local/lib/python3.10/site-packages (from llama-stack-client) (2.2.3)\n",
      "lready satisfied: pyaml in /home/amd/.local/lib/python3.10/site-packages (from llama-stack-client) (25.1.0)\n",
      "m llama-stack-client) (1.3.1): sniffio in /home/amd/.local/lib/python3.10/site-packages (fro\n",
      "/site-packages (from llama-stack-client) (4.67.1).local/lib/python3.10\n",
      ",>=4.7 in /usr/local/lib/python3.10/dist-packages (from llama-stack-client) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/amd/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->llama-stack-client) (1.2.2)\n",
      "r/lib/python3/dist-packages (from anyio<5,>=3.5.0->llama-stack-client) (3.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx->llama-stack) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /home/amd/.local/lib/python3.10/site-packages (from httpx->llama-stack) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/amd/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-stack) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2>=3.1.6->llama-stack) (2.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/amd/.local/lib/python3.10/site-packages (from pydantic>=2->llama-stack) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/amd/.local/lib/python3.10/site-packages (from pydantic>=2->llama-stack) (2.27.2)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /usr/lib/python3/dist-packages (from blobfile->llama-stack) (3.11.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/lib/python3/dist-packages (from blobfile->llama-stack) (1.26.5)\n",
      "te-packages (from blobfile->llama-stack) (5.3.1)e/amd/.local/lib/python3.10/si\n",
      "r/local/lib/python3.10/dist-packages (from blobfile->llama-stack) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->llama-stack) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/amd/.local/lib/python3.10/site-packages (from huggingface-hub->llama-stack) (24.2)\n",
      "3/dist-packages (from huggingface-hub->llama-stack) (5.4.1)on\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/amd/.local/lib/python3.10/site-packages (from jsonschema->llama-stack) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/amd/.local/lib/python3.10/site-packages (from jsonschema->llama-stack) (2024.10.1)\n",
      "ng>=0.28.4 in /home/amd/.local/lib/python3.10/site-packages (from jsonschema->llama-stack) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/amd/.local/lib/python3.10/site-packages (from jsonschema->llama-stack) (0.22.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/amd/.local/lib/python3.10/site-packages (from pandas->llama-stack-client) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/amd/.local/lib/python3.10/site-packages (from pandas->llama-stack-client) (2.9.0.post0)\n",
      "sr/lib/python3/dist-packages (from pandas->llama-stack-client) (2022.1)\n",
      "d: tzdata>=2022.7 in /home/amd/.local/lib/python3.10/site-packages (from pandas->llama-stack-client) (2025.1)\n",
      "Requirement already satisfied: wcwidth in /home/amd/.local/lib/python3.10/site-packages (from prompt-toolkit->llama-stack) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/amd/.local/lib/python3.10/site-packages (from requests->llama-stack) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/amd/.local/lib/python3.10/site-packages (from rich->llama-stack) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/amd/.local/lib/python3.10/site-packages (from rich->llama-stack) (2.19.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/amd/.local/lib/python3.10/site-packages (from tiktoken->llama-stack) (2024.11.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/amd/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing dependencies of devscripts: Invalid version: '2.22.1ubuntu1'\u001b[0m\u001b[33m\n",
      "0m\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install llama-stack llama-stack-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f232204-a424-41dd-966f-3c3d13b0901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_stack                       0.2.1\n",
      "llama_stack_client                0.2.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip list | grep llama_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3058a532-9cdc-45a9-9a42-a69908712fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama-stack'...\n",
      "Updating files: 100% (924/924), done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git clone https://github.com/meta-llama/llama-stack.git\n",
    "# Copy the template yaml of remote-vllm distro \n",
    "cp ./llama-stack/llama_stack/templates/remote-vllm/run.yaml .\n",
    "cp ./llama-stack/llama_stack/templates/remote-vllm/run-with-safety.yaml ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748cbd0b-14f8-4359-bcff-891ecf3a5993",
   "metadata": {},
   "source": [
    "### Running llama-stack\n",
    "\n",
    "Llama-stcke release the distriution-remote-vllm docekr image run as the frontend while the vLLM serve at the backend. We should set the ports and the models of the containers of vllm serve to start the container of llama-stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "464f32c6-3d6c-4f93-8377-6127377f089a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Published ports are discarded when using host network mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2619ffa6d1bbc7ed18da09c0444ccbfb73868e4910aec51bafeb5f89d13e08a0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export INFERENCE_PORT=8080\n",
    "export INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct\n",
    "export LLAMA_STACK_PORT=8321\n",
    "export SAFETY_PORT=8081\n",
    "export SAFETY_MODEL=meta-llama/Llama-Guard-3-1B\n",
    "\n",
    "docker run -d --rm \\\n",
    "  --network=host \\\n",
    "  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n",
    "  -v ~/.llama:/root/.llama \\\n",
    "  -v ./run-with-safety.yaml:/root/my-run.yaml \\\n",
    "  --name llama-stack-distro \\\n",
    "  llamastack/distribution-remote-vllm \\\n",
    "  --config /root/my-run.yaml \\\n",
    "  --port $LLAMA_STACK_PORT \\\n",
    "  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n",
    "  --env VLLM_URL=http://0.0.0.0:$INFERENCE_PORT/v1 \\\n",
    "  --env SAFETY_MODEL=$SAFETY_MODEL \\\n",
    "  --env SAFETY_VLLM_URL=http://0.0.0.0:$SAFETY_PORT/v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b6a1e-e1c4-4f32-98d2-6cfdcf2baa8c",
   "metadata": {},
   "source": [
    "Now, we have three containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cbc6648-50e1-4cac-808f-02546b0408a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waiting for the container start\n",
    "!sleep 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8df12f9e-e216-42aa-9565-acb63d9c00b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                 COMMAND                  CREATED              STATUS              PORTS                                       NAMES\n",
      "2619ffa6d1bb   llamastack/distribution-remote-vllm   \"python -m llama_sta…\"   About a minute ago   Up About a minute                                               llama-stack-distro\n",
      "8f924b37c229   rocm/vllm-dev:main                    \"python -m vllm.entr…\"   7 minutes ago        Up 7 minutes        0.0.0.0:8081->8081/tcp, :::8081->8081/tcp   rocm-vllm-guard\n",
      "1b590b7b6342   rocm/vllm-dev:main                    \"python -m vllm.entr…\"   7 minutes ago        Up 7 minutes        0.0.0.0:8080->8080/tcp, :::8080->8080/tcp   rocm-vllm-provider\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5ca1f-dd52-4ec2-a6ff-ab2ce5fe89ff",
   "metadata": {},
   "source": [
    "### Use llama-stack client CLI\n",
    "\n",
    "We could use the client side CLI to access the llama-stack service. You could more details from https://llama-stack.readthedocs.io/en/latest/references/llama_stack_client_cli_reference.html ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca9c4231-7dd0-4453-b57c-6fda3e12b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAvailable Models\u001b[0m\n",
      "\n",
      "┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓\n",
      "┃\u001b[1m  \u001b[0m┃\u001b[1m \u001b[0m\u001b[1midentifier                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mprovider_resource_id            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m  \u001b[0m┃\u001b[1m  \u001b[0m┃\n",
      "┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩\n",
      "│\u001b[34m  \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36mall-MiniLM-L6-v2                \u001b[0m\u001b[1;36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mall-MiniLM-L6-v2                \u001b[0m\u001b[33m \u001b[0m│\u001b[35m  \u001b[0m│\u001b[32m  \u001b[0m│\n",
      "├──┼──────────────────────────────────┼──────────────────────────────────┼──┼──┤\n",
      "│\u001b[34m  \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36mmeta-llama/Llama-3.2-3B-Instruct\u001b[0m\u001b[1;36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mmeta-llama/Llama-3.2-3B-Instruct\u001b[0m\u001b[33m \u001b[0m│\u001b[35m  \u001b[0m│\u001b[32m  \u001b[0m│\n",
      "├──┼──────────────────────────────────┼──────────────────────────────────┼──┼──┤\n",
      "│\u001b[34m  \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36mmeta-llama/Llama-Guard-3-1B     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33mmeta-llama/Llama-Guard-3-1B     \u001b[0m\u001b[33m \u001b[0m│\u001b[35m  \u001b[0m│\u001b[32m  \u001b[0m│\n",
      "└──┴──────────────────────────────────┴──────────────────────────────────┴──┴──┘\n",
      "\n",
      "Total models: \u001b[1;36m3\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!llama-stack-client models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb08ece-4726-4ab8-93a1-193f9050ec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mAPI         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mProvider ID           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mProvider Type                 \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│ inference    │ vllm-inference         │ remote::vllm                   │\n",
      "│ inference    │ vllm-safety            │ remote::vllm                   │\n",
      "│ inference    │ sentence-transformers  │ inline::sentence-transformers  │\n",
      "│ vector_io    │ faiss                  │ inline::faiss                  │\n",
      "│ safety       │ llama-guard            │ inline::llama-guard            │\n",
      "│ agents       │ meta-reference         │ inline::meta-reference         │\n",
      "│ eval         │ meta-reference         │ inline::meta-reference         │\n",
      "│ datasetio    │ huggingface            │ remote::huggingface            │\n",
      "│ datasetio    │ localfs                │ inline::localfs                │\n",
      "│ scoring      │ basic                  │ inline::basic                  │\n",
      "│ scoring      │ llm-as-judge           │ inline::llm-as-judge           │\n",
      "│ scoring      │ braintrust             │ inline::braintrust             │\n",
      "│ telemetry    │ meta-reference         │ inline::meta-reference         │\n",
      "│ tool_runtime │ brave-search           │ remote::brave-search           │\n",
      "│ tool_runtime │ tavily-search          │ remote::tavily-search          │\n",
      "│ tool_runtime │ code-interpreter       │ inline::code-interpreter       │\n",
      "│ tool_runtime │ rag-runtime            │ inline::rag-runtime            │\n",
      "│ tool_runtime │ model-context-protocol │ remote::model-context-protocol │\n",
      "│ tool_runtime │ wolfram-alpha          │ remote::wolfram-alpha          │\n",
      "└──────────────┴────────────────────────┴────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!llama-stack-client providers list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398d50e-cfed-4166-a430-20d8c08c3114",
   "metadata": {},
   "source": [
    "We could request inference by the CLI like that,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "403744e0-ae74-4676-8e17-e3992ccdd832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mChatCompletionResponse\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[33mcompletion_message\u001b[0m=\u001b[1;35mCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
      "        \u001b[33mcontent\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"name\": \"print\", \"parameters\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"f\": \"Why was the math book \u001b[0m\n",
      "\u001b[32msad? Because it had too many problems.\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
      "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
      "        \u001b[33mstop_reason\u001b[0m=\u001b[32m'end_of_turn'\u001b[0m,\n",
      "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
      "    \u001b[1m)\u001b[0m,\n",
      "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
      "    \u001b[33mmetrics\u001b[0m=\u001b[1m[\u001b[0m\n",
      "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'prompt_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m14\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
      "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'completion_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m38\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
      "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'total_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m52\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[1m]\u001b[0m\n",
      "\u001b[1m)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!llama-stack-client inference chat-completion --message \"tell me a joke\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded78335-fe58-47eb-a909-b6402c9d36a2",
   "metadata": {},
   "source": [
    "### Use Python Client SDK\n",
    "\n",
    "The Llama Stack provide Python Client SDK for developping the application. Here is a example to use the API to do the inference.\n",
    "Here is the simple code refert to the one from https://llama-stack.readthedocs.io/en/latest/getting_started/index.html#test-basic-inference ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4125b569-e426-4c82-bd3e-8c9aea952f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > inference.py << EOF\n",
    "# inference.py\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=f\"http://localhost:8321\")\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Select the first LLM\n",
    "llm = next(m for m in models if m.model_type == \"llm\")\n",
    "model_id = llm.identifier\n",
    "\n",
    "print(\"Model:\", model_id)\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about coding\"},\n",
    "    ],\n",
    ")\n",
    "print(response.completion_message.content)\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ae94876-72b8-4390-ba12-09e5799e4ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct\n",
      "{\"name\": \"haiku\", \"parameters\": {\"c\": \"code\", \"t\": \"typing\", \"s\": \"silent\"}}\n"
     ]
    }
   ],
   "source": [
    "!python inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37e986-1a95-4e93-82a6-ec3ff6f286df",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ddf84c0-4c5d-4e1d-b526-c84963fd0f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-stack-distro\n",
      "rocm-vllm-guard\n",
      "rocm-vllm-provider\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf llama-stack\n",
    "rm run.yaml\n",
    "rm run-with-safety.yaml\n",
    "rm inference.py\n",
    "docker stop llama-stack-distro\n",
    "docker stop rocm-vllm-guard\n",
    "docker stop rocm-vllm-provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16d24-ae68-4290-a359-41f9652f7477",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] [Llama Stack Docunmentation](https://llama-stack.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "[2] [AMD ROCm™ documentation](https://ROCm™.docs.amd.com/en/latest/)\n",
    "\n",
    "[3] [How to Build a vLLM Container for Inference and Benchmarking](https://ROCm™.blogs.amd.com/software-tools-optimization/vllm-container/README.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
